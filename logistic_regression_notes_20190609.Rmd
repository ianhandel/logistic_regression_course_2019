---
title: "Logistic regression - introduction"
author: "Ian Handel"
date: "`r Sys.Date()`"
output: tufte::tufte_handout
---

```{r, include= FALSE}
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(broom)
library(sjPlot)

set.seed(42)
opts_chunk$set(comment = "", error = FALSE)
library(tufte)
purple <- "#CAB2D6"
```

# Introduction

These notes are a basic introduction to binary logistic regression used to analyse data with binary outcomes. In these notes we'll aim to cover the following learning outcomes:

* __Refresher__ on logs, odds, probability and linear regression
* Understand why linear regression not sensible for __binary data__
* Explain how __logit__ and binomial model let us __extend linear regression__
* Be able to run a __simple logistic regression in R__
* Be able to explain basic R glm __output__
* Be able to explain __estimates__ with categorical and continuous variables
* Explain __significance test results__ on variables
* Introduce some basic ideas for __selecting variables and models__
* __Things to watch out for!__
* __Know where to go next!__

# Prerequisites

We'll cover a couple of background topics but to follow these notes you'll need to be able to load packages in R, run simple code in R and have a basic understanding of linear regression and statistical hypothesis tests.

# Running the example code

To run the R code in these notes you'll need to start an rstudio project, load in the example data-set and have a few packages downloaded and loaded into your R session. 

To download the packages (if you don't have them already) use...

```{r eval = FALSE}
install.packages("tidyverse")
install.packages("boot")
install.packages("broom")
install.packages("skimr")
install.packages("sjPlot")
install.packages("kableExtra")
```

Start a new project in rstudio and in an rscript use the floowing code to load the libraries you need and to import/load the data...

Loading the packages...

```{r eval = FALSE}
library(tidyverse)
library(boot)
library(broom)
library(skimr)
library(sjPlot)
library(kableExtra)
```

Loading the data  (from the csv file on Basecamp)...

```{r, eval = FALSE}
dat <- read_csv("logreg_data_01_20190530.csv")

```

```{r, include = FALSE}
library(tidyverse)
library(boot)
library(broom)
library(skimr)
library(sjPlot)
```

```{r, include = FALSE}
dat <- read_csv("logreg_data_01_20190530.csv")
```

This dataset describes `r nrow(dat)` animals giving their weight in kg, age in days, supplement levels (mg), sex, region where they liver (A, B, C or D) and whether they were treated with anthelmintics or not. It's a dataset made up for this course by the way!

# Revision / background topics

## Logarithms ('logs')

Skip this if you are happy with logs (including base 'e')

'Logs' are a mathematical function that changes a number. They take the form $log_b(x)=y$. What this means is $b$ to the power $y$ will give us $x$. It's easier to understand with examples. Let's start with base 10....

$$log_{10}(10)=1$$

$$log_{10}(1000)=3$$

$$log_{10}(0.01)=-2$$

So the logs of all the numbers in brackets are the number you'd need to raise 10 to to get them.

We can have other bases e.g. $e$., $e$ is a special mathematical constant that feautres a lot behind the scene in statistics it's roughly 2.718...

$$log_e(2.718)\simeq1$$

'Inverse logs' let us turn logs back into the original number. We simply raise the 'base' of our logs to the number we what to invert and we end up with the original number. So $log_{10}(1000)=3$ and $10^3=1000$...

One feature of logs is that adding the logs of two numbers is equivalent to multiplying the numbers...

$$100 \times 1000 = 100000$$

$$log_{10}(100) + log_{10}(1000) = log_{10}(100000)$$

Because $log_{10}(100) = 2$, $log_{10}(1000)=3$ and $log_{10}(100000)=5$

If this all seems a bit too much don't worry. Just remember that adding logs is like multiplying numbers and you'll be fine!


# Odds and probability

__Probabilities__ have values from 0 ('never happens') to 1 ('always happens')

__'events of interest' รท 'all events'__


What is the probability that a fair coin  lands on heads?


$$1/2 = 0.5$$

What is the probability that a 6 sided die lands on 4?

$$1/6 \simeq 0.166$$

__Odds__ have values from 0 ('never happen') to infinity ('always happens')

__'events of interest' รท 'other events'__

What are the odds that a fair coin  lands on heads?

$$1/1 = 1$$

What are the odds that a 6 sided die lands on 4?

$$1/5 = 0.2$$

\newpage

# Linear regression

Remember that linear regression is a statistical method that lets us understand and predict numerical outcomes using one or more predictor variables. The predictor variables may be numerical or categorical. Linear regression also assumes there's a linear i.e. straight line relationship between the predictor numerical variable sand the outcome. Using the data set we have loaded we can plot weight against age and sex and see that there looks to be a linear relationship between age and weight for each sex...

```{r, fig.width=4, fig.height=3}
fig1 <- ggplot(dat) +
  aes(age, weight, colour = sex) +
  geom_point(shape = 1) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  labs(title = "Weight vs Age of the animals",
       subtitle = "They get bigger as they age\nand males are heavier",
       x = "Age (days)",
       y = "Weight (Kg)",
       colour = "Sex")

print(fig1)
```

In R we can use the ```lm()``` function to fit a linear model to this data. Normally we store the results of using the function in an R object and look at it using ``summary()`. We can also get a tidier output using ```get_model_data()``` from the ```sjPlot``` package. Here we make a linear model predicting the animal's weight from their age (a numerical variable) and their sex (a categorical variable).

```{r}
mod1 <- lm(weight ~ age + sex, data = dat)
```

Using ```summary()``` from base-R...

```{r}
summary(mod1)
```

Using ```get_model_data()``` from ```sjPlot``` package and selecting the output columns we want...

```{r}
get_model_data(mod1) %>%
  select(term:p.stars) %>% 
  print()
```

The analysis suggestes that both __age__ and __sex__ are significant predcitors of weight. Weight increasing by about `r round(coef(mod1)[["age"]], 2)` Kg per day of age and that males are about `r round(coef(mod1)[["sexmale"]], 1)` Kg heavier than females.

\newpage

# Analysing binary data

Binary data common in epidemiology e.g. disease status (diseased or healthy), life (alive or dead) etc. In the example dataset we have the column `status` where animals can be either __healthy__ or __diseased__.

```{r, echo = FALSE}
dat %>% 
  sample_n(6) %>% 
  kable(format = "latex", caption = "Example data")
```

As we know that some animals were treated with an anthelmintic and some were not it may be interesting to look at the number of healhty and diseased animals in the treated and control (untreated) groups...

```{r, echo = FALSE, fig.cap="Disease status vs treatment"}
dat %>% 
  count(treatment, status) %>% 
  spread(status, n) %>% 
  kable(format = "latex")
```

```{r}
with(dat,
     {{fisher.test(status, treatment)}})
```



# Multivariable analysis

How about recoding the outcome as 0/1?

```{r, echo = FALSE}
set.seed(42)
dat <- dat %>%
  mutate(status01 = as.numeric(status == "diseased"))

dat %>% 
  sample_n(6) %>% 
  kable(format = "html", caption = "Example data")
```

Then use linear regression...



# Linear regression 1

```{r, echo = FALSE, fig.height=3}
p <- ggplot(dat) +
  aes(age, status01, colour = treatment) +
  geom_point(shape = 1) +
  theme_bw(base_size = 16) +
  scale_y_continuous(limits = c(-0.2, 1.0), breaks = c(0,0.5,1)) +
  labs(title = "Status vs Age and treatment",
       x = "Age (days)",
       y = "Outcome",
       colour = "Treatment")

p
```



# Linear regression 2

```{r, echo = FALSE, fig.width=4, fig.height=3}
p +
  geom_smooth(method = "lm", se = FALSE, lwd = 0.5)
```



## Problems



-predicts (impossible) intermediate values



-can predict <0 and >1



# So how do we fix this?

__Linear regression does this...__

$weight \sim \beta_0 + \beta_1 age + \beta_2 sex + \epsilon$

or in english...

The outcome, $weight$, is related to the predictors

by one or more straight lines.

```{r, fig.width=5, fig.height=4}
print(fig1)
```

__For binary data we want__

Our outcome to be 0 or 1

So rather than modelling the outcome.

We model the __probability__ of something e.g. being diseased...

# The logistic bit...

Linear regression models model numbers, any numbers!

Probabilities go from...

0 to 1

So we need to turn any number into 0 - 1

```{r, echo = FALSE, fig.height=3, fig.width=4, fig.align='center'}
tibble(y = seq(-10, 10, 0.1),
       prob = boot::inv.logit(y)) %>% 
  ggplot() +
  aes(y, prob) +
  geom_line(colour = purple, lwd = 2) +
  labs(x = "linear regression value",
       y = "probability",
       title = "The inverse logit") +
  theme_bw(base_size = 12)

```

In fact the regression value is the log of the odds of the outcome.

#The logistic bit 2

So we have an outcome, e.g. being diseased vs healthy, that is coded 0 or 1

And our model is

$$ log_e(\frac{prob}{1 - prob}) \sim \beta_0 + \beta_1 age + \beta_2 treatment) $$

or in english


__The log of the odds of an animal being diseased are modelled by a linear combination of the predictor variables__


# Worked example in R



# R code for logistic regression

```{r, eval= FALSE}
head(dat)
```


```{r, echo = FALSE}
head(dat) %>% 
  kable(format = "latex") %>% 
  kable_styling(font_size = 8)
```

A linear model of weight

```{r, eval = FALSE}
mod_weight <- lm(weight ~ age + sex, data = dat)
```

A logistic regression model of disease status

```{r}
mod_disease <- glm(status01 ~ treatment + age, family = binomial, data = dat)
```



# The output

```{r}
print(summary(mod_disease), digits = 3)
```




# The output

```{r, highlight.output = c(10, 11, 12, 13, 14)}
print(summary(mod_disease), digits = 3)
```




# The output

Lets get 'tidy output...

```{r}
tidy(mod_disease) #tidy from the broom package
```



# odds ratios

The estimates = log(odds ratios)

i.e. 

$$\frac{odds\ of\ outcome\ if\ have\ factor}{odds\ of\ outcome\ if\ dont\ have\ factor}$$

So we get odds ratios by 'inverse logging them'.

We can remove the intercept.

```{r}
tidy(mod_disease) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(OR = exp(estimate))
```



# A results table

```{r, eval = FALSE}
tidy(mod_disease) %>%
  mutate(OR = exp(estimate)) %>% 
    bind_cols(exp(confint_tidy(mod_disease)) %>% 
    as_tibble()
  ) %>% 
  filter(term != "(Intercept)") %>% 
  select(term, OR, `conf.low`, `conf.high`, p.value)
```

```{r, echo = FALSE, message=FALSE}
tidy(mod_disease) %>%
  mutate(OR = exp(estimate)) %>% 
    bind_cols(exp(confint_tidy(mod_disease)) %>% 
    as_tibble()
  ) %>% 
  filter(term != "(Intercept)") %>% 
  select(term, OR, `conf.low`, `conf.high`, p.value) %>% 
  kable(format = "html", digits = 3)
```




But what does it mean?



# Interpreting the odds ratios


```{r, echo = FALSE, message=FALSE}
tidy(mod_disease) %>%
  mutate(OR = exp(estimate)) %>% 
    bind_cols(exp(confint(mod_disease)) %>% 
    as_tibble()
  ) %>% 
  filter(term != "(Intercept)") %>% 
  select(term, OR, `2.5 %`, `97.5 %`, p.value) %>% 
  kable(format = "latex", digits = 3)
```


## Odds ratios __multiply__



## Categorical predictors

How many times greater the odds of outcome are __if__ the risk factor (etc) is present.



So for the treatment variable (which can be control or treatment) the odds of disease if treated are `r round(exp(coef(mod_disease))[[2]], 3)` __times greater__ than if untreated (control).



# Interpreting the odds ratios

```{r, echo = FALSE, message=FALSE}
tidy(mod_disease) %>%
  mutate(OR = exp(estimate)) %>% 
    bind_cols(exp(confint(mod_disease)) %>% 
    as_tibble()
  ) %>% 
  filter(term != "(Intercept)") %>% 
  select(term, OR, `2.5 %`, `97.5 %`, p.value) %>% 
  kable(format = "latex", digits = 3)
```


## Odds ratios __multiply__


## Numerical predictors

How many times greater the odds of outcome are for __each unit change__ in the variable



So for the age variable the odds of disease are `r round(exp(coef(mod_disease))[[3]], 3)` __times greater__ for each day older.

So for 3 days it's `r round(exp(coef(mod_disease))[[3]], 3)` x `r round(exp(coef(mod_disease))[[3]], 3)` x `r round(exp(coef(mod_disease))[[3]], 3)` $\simeq$ `r round(exp(coef(mod_disease))[[3]], 3)^3 %>% round(3)`.




# Things to watch out for

## Factor levels

How does R know if you are predicting 'healthy' or 'diseased'?



## Perfect predictors

E.g. all the males are diseased and all the females are healthy



## Linear on logit

Disease risk might go up and then down



# Model selection - a blank page



# More help

Veterinary Epi Reasearch - Ian Dahoo














