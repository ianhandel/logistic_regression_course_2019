---
title: "Logistic regression - introduction"
author: "Ian Handel"
date: "`r Sys.Date()`"
output: tint::tintPdf
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tint)
# invalidate cache when the package version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tint'))
options(htmltools.dir.version = FALSE)
```


```{r, include= FALSE}
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(broom)
library(sjPlot)
library(modelr)

set.seed(42)
opts_chunk$set(comment = "", error = FALSE)
library(tufte)
purple <- "#CAB2D6"
```

# Introduction

`r newthought('These notes')` are a basic introduction to binary logistic regression used to analyse data with binary outcomes. In these notes we'll aim to cover the following learning outcomes:

* __Refresher__ on logs, odds, probability and linear regression
* Understand why linear regression not sensible for __binary data__
* Explain how __logit__ and binomial model let us __extend linear regression__
* Be able to run a __simple logistic regression in R__
* Be able to explain basic R glm __output__
* Be able to explain __estimates__ with categorical and continuous variables
* Explain __significance test results__ on variables
* Introduce some basic ideas for __selecting variables and models__
* __Things to watch out for!__
* __Know where to go next!__

# Prerequisites

We'll cover a couple of background topics but to follow these notes you'll need to be able to load packages in R, run simple code in R and have a basic understanding of linear regression and statistical hypothesis tests.

# Running the example code

To run the R code in these notes you'll need to start an rstudio project, load in the example data-set and have a few packages downloaded and loaded into your R session. 

To download the packages (if you don't have them already) use... `r margin_note("You can also use the install button in the RStudio packages tab to install these")`

```{r eval = FALSE}
install.packages("tidyverse")
install.packages("boot")
install.packages("broom")
install.packages("skimr")
install.packages("sjPlot")
install.packages("kableExtra")
```

Start a new project in rstudio and in an rscript use the floowing code to load the libraries you need and to import/load the data...

Loading the packages...

```{r eval = FALSE}
library(tidyverse)
library(boot)
library(broom)
library(skimr)
library(sjPlot)
library(kableExtra)
```

Loading the data  (from the csv file on Basecamp)... `r margin_note("Once loaded you should see the a 'dat' object in the RStudio environment tab (usually in the top-right of your screen)")`

```{r, eval = FALSE}
dat <- read_csv("logreg_data_01_20190530.csv")

```

```{r, include = FALSE}
library(tidyverse)
library(boot)
library(broom)
library(skimr)
library(sjPlot)
```

```{r, include = FALSE}
dat <- read_csv("logreg_data_01_20190530.csv")
```

This dataset describes `r nrow(dat)` animals giving their weight in kg, age in days, supplement levels (mg), sex, region where they liver (A, B, C or D) and whether they were treated with anthelmintics or not. It's a dataset made up for this course by the way!

# Revision / background topics

## Logarithms ('logs')

Skip this if you are happy with logs (including base 'e')

'Logs' are a mathematical function that changes a number. They take the form $log_b(x)=y$. What this means is $b$ to the power $y$ will give us $x$. It's easier to understand with examples. Let's start with base 10....

$$log_{10}(10)=1$$

$$log_{10}(1000)=3$$

$$log_{10}(0.01)=-2$$

So the logs of all the numbers in brackets are the number you'd need to raise 10 to to get them.

We can have other bases e.g. $e$., $e$ is a special mathematical constant that feautres a lot behind the scene in statistics it's roughly 2.718...

$$log_e(2.718)\simeq1$$

'Inverse logs' let us turn logs back into the original number. We simply raise the 'base' of our logs to the number we what to invert and we end up with the original number. So $log_{10}(1000)=3$ and $10^3=1000$...

One feature of logs is that adding the logs of two numbers is equivalent to multiplying the numbers...

$$100 \times 1000 = 100000$$

$$log_{10}(100) + log_{10}(1000) = log_{10}(100000)$$

Because $log_{10}(100) = 2$, $log_{10}(1000)=3$ and $log_{10}(100000)=5$

If this all seems a bit too much don't worry. Just remember that adding logs is like multiplying numbers and you'll be fine!


## Odds and probability


>__Probabilities__ have values from 0 ('never happens') to 1 ('always happens')


>__'events of interest' รท 'all events'__.


What is the probability that a fair coin  lands on heads?


$$1/2 = 0.5$$

What is the probability that a 6 sided die lands on 4?

$$1/6 \simeq 0.166$$

>__Odds__ have values from 0 ('never happen') to infinity ('always happens')


>__'events of interest' รท 'other events'__.


What are the odds that a fair coin  lands on heads?

$$1/1 = 1$$

What are the odds that a 6 sided die lands on 4?

$$1/5 = 0.2$$

\newpage

## Linear regression

Remember that linear regression is a statistical method that lets us understand and predict numerical outcomes using one or more predictor variables. The predictor variables may be numerical or categorical. Linear regression also assumes there's a linear i.e. straight line relationship between the predictor numerical variable sand the outcome. Using the data set we have loaded we can plot weight against age and sex and see that there looks to be a linear relationship between age and weight for each sex...

```{r, fig.margin=TRUE, fig.width=5, fig.height=4, fig.cap = "Weight vs Age of the animals"}
ggplot(dat) +
  aes(age, weight, colour = sex) +
  geom_point(shape = 1) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  labs(x = "Age (days)",
       y = "Weight (Kg)",
       colour = "Sex")
```

In R we can use the ```lm()``` function to fit a linear model to this data. Normally we store the results of using the function in an R object and look at it using ``summary()`. We can also get a tidier output using ```get_model_data()``` from the ```sjPlot``` package. Here we make a linear model predicting the animal's weight from their age (a numerical variable) and their sex (a categorical variable).

```{r}
mod1 <- lm(weight ~ age + sex, data = dat)
```

Using ```summary()``` from base-R...


\small
```{r, echo=FALSE}
opt <-options(width = 120)
```

```{r}
summary(mod1)
```

```{r, echo=FALSE}
options(opt)
```
\normalsize

Using ```get_model_data()``` from ```sjPlot``` package and selecting the output columns we want...

```{r, echo=FALSE}
opt <-options(width = 120)
```

\small
```{r}
get_model_data(mod1) %>%
  select(term:p.stars) %>% 
  print()
```
\normalsize

```{r, echo=FALSE}
options(opt)
```

The analysis suggestes that both __age__ and __sex__ are significant predictors of weight. Weight increasing by about `r round(coef(mod1)[["age"]], 2)` Kg per day of age and that males are about `r round(coef(mod1)[["sexmale"]], 1)` Kg heavier than females. Right - now to look at binary (yes/no) outcome data and logistic regression...

\newpage

# Analysing binary data
Binary data is common in epidemiological studies e.g. disease status (diseased or healthy), life (alive or dead) etc. In the example dataset we have the column `status` where animals can be either __healthy__ or __diseased__.
\break
```{r, echo = FALSE}
dat %>% 
  sample_n(6) %>% 
  kable(format = "latex", caption = "Example data")
```
\break
As we know that some animals were treated with an anthelmintic and some were not it may be interesting to look at the number of healthy and diseased animals in the treated and control (untreated) groups.
\break
```{r, echo = FALSE, fig.cap="Disease status vs treatment"}
dat %>% 
  count(treatment, status) %>% 
  spread(status, n) %>% 
  kable(format = "latex")
```
\break
Approximately `r dat %>% filter(treatment == "control") %>% summarise(mean(status == "diseased") * 100) %>% round()`% of the untreated (control) animals were diseased and `r dat %>% filter(treatment == "treated") %>% summarise(mean(status == "diseased") * 100) %>% round()`% of the treated animals were diseased. We can use a 'Fisher's Exact test' to test if the proportion of diseased anaimals in the treatment vs control groups are significantly different. This test also estimates the odds ratio of disease between the two groups. `r margin_note("You can use ?fisher.test in the RStudio console to get help on this function. Here we have also used the with() function to make it easier to use the 'dat' dataset and refer to the columns in the fisher.test() code")`
\break
```{r}
with(dat,
     {{fisher.test(status, treatment)}})
```

The Fisher's exact test reports a p-value, in this case `r with(dat,{{fisher.test(status, treatment)}})$p.value %>% round(3)`. If this is less than your chosen critical p-value you have evidence that the proprtion of diaased animals is significantly different between the treated and untreated (control) animal groups. Fisher's Exact test is useful for comaprions between a binary outcome and a single, binary predciotr. However we may want to look at the association between several predcitors and a binary outcome in one go. This is called __multivariable__ analysis and you'll have met it in linear regression. We can't look at this with a simple test so need to consider regression approaches...

\newpage

# Multivariable analysis

We could convert out `status` column to a number coding healhty animals as __0__ and diseased animals as __1__ and then use conventional linear regression with the variables we want to consider as predictors. The following code will make a new column, `status01` recoding status to a number. `r margin_note("To understand how this works remember that == in R compares things and if they are the same returns TRUE andf if they aren't returns FALSE. Then as.numeric() will convert FALSE to a 0 and TRUE to 1. It's a handy way of making binary text /factor data into 0's and 1's")`

```{r, eval = FALSE}
dat <- dat %>%
  mutate(status01 = as.numeric(status == "diseased"))
```

```{r, echo = FALSE}
dat <- dat %>%
  mutate(status01 = as.numeric(status == "diseased"))

dat %>% 
  head(4) %>% 
  kable(format = "latex")
```
\break
Now we have converted the binary data into a number (0 or 1) we __could__ try using linear regression. First, to help understand what we are trying here, we can plot the outcome against the treatment group and age of the animals we see this...

```{r, fig.cap = "Status vs Age and treatment", fig.height=3, fig.width=6, fig.fullwidth = TRUE, out.height='30%'}
ggplot(dat) +
  aes(age, status01, colour = treatment) +
  geom_point(shape = 1) +
  theme_bw(base_size = 16) +
  labs(x = "Age (days)",
       y = "Outcome",
       colour = "Treatment")
```
\break
Linear regression will attempt to fit straight lines of 'prediction' to the control and treatment data.

```{r, echo=FALSE}
opt <-options(width = 120)
```

```{r}
mod_lin <- lm(status01 ~ age + treatment, data = dat)

get_model_data(mod_lin) %>% 
  select(term:p.stars)
```

```{r, echo = FALSE}
options(opt)
```
\break
Before we get too excited by this let's look at the fitted prediction line on the original plot.

```{r, fig.cap = "Status vs Age and treatment (with linear model)", fig.height=3, fig.width=6, fig.fullwidth = TRUE, out.height='30%'}
dat %>% 
  modelr::add_predictions(mod_lin) %>%
  ggplot() +
  aes(age, status01, colour = treatment) +
  geom_point(shape = 1) +
  geom_line(aes(y = pred)) +
  theme_bw(base_size = 16) +
  labs(x = "Age (days)",
       y = "Outcome",
       colour = "Treatment")

```
\break
Our outomce can only be __0__ i.e. __healhty__ or __1__ i.e. __diseased__. However our linear mode is predicting numbers inbetween. In some circumstances trying to use a linear model to predict binary 0/1/ data will even give values less than zero and more than one. We need to re-think the regression approach so that our model makes sense with an outcomethat can only be __0__ or __1__ (or healhty / diseased etc) 




