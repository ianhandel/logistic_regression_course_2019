---
title: "Logistic regression - introduction"
author: "Ian Handel"
date: "`r Sys.Date()`"
output: tint::tintPdf
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tint)
# invalidate cache when the package version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tint'))
options(htmltools.dir.version = FALSE)
```


```{r, include= FALSE}
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(broom)
library(sjPlot)
library(modelr)

set.seed(42)
opts_chunk$set(comment = "", error = FALSE)
library(tufte)
purple <- "#CAB2D6"
```

# Introduction

`r newthought('These notes')` are a basic introduction to binary logistic regression used to analyse data with binary outcomes. In these notes we'll aim to cover the following learning outcomes:

* __Refresher__ on logs, odds, probability and linear regression
* Understand why linear regression not sensible for __binary data__
* Explain how __logit__ and binomial model let us __extend linear regression__
* Be able to run a __simple logistic regression in R__
* Be able to explain basic R glm __output__
* Be able to explain __estimates__ with categorical and continuous variables
* Explain __significance test results__ on variables
* Introduce some basic ideas for __selecting variables and models__
* __Things to watch out for!__
* __Know where to go next!__

# Prerequisites

We'll cover a couple of background topics but to follow these notes you'll need to be able to load packages in R, run simple code in R and have a basic understanding of linear regression and statistical hypothesis tests.

# Running the example code

To run the R code in these notes you'll need to start an rstudio project, load in the example data-set and have a few packages downloaded and loaded into your R session. 

To download the packages (if you don't have them already) use... `r margin_note("You can also use the install button in the RStudio packages tab to install these")`

```{r eval = FALSE}
install.packages("tidyverse")
install.packages("boot")
install.packages("broom")
install.packages("skimr")
install.packages("sjPlot")
install.packages("kableExtra")
```

Start a new project in rstudio and in an rscript use the floowing code to load the libraries you need and to import/load the data...

Loading the packages...

```{r eval = FALSE}
library(tidyverse)
library(boot)
library(broom)
library(skimr)
library(sjPlot)
library(kableExtra)
```

Loading the data  (from the csv file on Basecamp)... `r margin_note("Once loaded you should see the a 'dat' object in the RStudio environment tab (usually in the top-right of your screen)")`

```{r, eval = FALSE}
dat <- read_csv("logreg_data_01_20190530.csv")

```

```{r, include = FALSE}
library(tidyverse)
library(boot)
library(broom)
library(skimr)
library(sjPlot)
```

```{r, include = FALSE}
dat <- read_csv("logreg_data_01_20190530.csv")
```

This dataset describes `r nrow(dat)` animals giving their weight in kg, age in days, supplement levels (mg), sex, region where they liver (A, B, C or D) and whether they were treated with anthelmintics or not. It's a dataset made up for this course by the way!

# Revision / background topics

## Logarithms ('logs')

Skip this if you are happy with logs (including base 'e')

'Logs' are a mathematical function that changes a number. They take the form $log_b(x)=y$. What this means is $b$ to the power $y$ will give us $x$. It's easier to understand with examples. Let's start with base 10....

$$log_{10}(10)=1$$

$$log_{10}(1000)=3$$

$$log_{10}(0.01)=-2$$

So the logs of all the numbers in brackets are the number you'd need to raise 10 to to get them.

We can have other bases e.g. $e$., $e$ is a special mathematical constant that feautres a lot behind the scene in statistics it's roughly 2.718...

$$log_e(2.718)\simeq1$$

'Inverse logs' let us turn logs back into the original number. We simply raise the 'base' of our logs to the number we what to invert and we end up with the original number. So $log_{10}(1000)=3$ and $10^3=1000$...

One feature of logs is that adding the logs of two numbers is equivalent to multiplying the numbers...

$$100 \times 1000 = 100000$$

$$log_{10}(100) + log_{10}(1000) = log_{10}(100000)$$

Because $log_{10}(100) = 2$, $log_{10}(1000)=3$ and $log_{10}(100000)=5$

If this all seems a bit too much don't worry. Just remember that adding logs is like multiplying numbers and you'll be fine!


## Odds and probability


>__Probabilities__ have values from 0 ('never happens') to 1 ('always happens')


>__'events of interest' รท 'all events'__.


What is the probability that a fair coin  lands on heads?


$$1/2 = 0.5$$

What is the probability that a 6 sided die lands on 4?

$$1/6 \simeq 0.166$$

>__Odds__ have values from 0 ('never happen') to infinity ('always happens')


>__'events of interest' รท 'other events'__.


What are the odds that a fair coin  lands on heads?

$$1/1 = 1$$

What are the odds that a 6 sided die lands on 4?

$$1/5 = 0.2$$

\newpage

## Linear regression

Remember that linear regression is a statistical method that lets us understand and predict numerical outcomes using one or more predictor variables. The predictor variables may be numerical or categorical. Linear regression also assumes there's a linear i.e. straight line relationship between the predictor numerical variable sand the outcome. Using the data set we have loaded we can plot weight against age and sex and see that there looks to be a linear relationship between age and weight for each sex...

```{r, fig.margin=TRUE, fig.width=5, fig.height=4, fig.cap = "Weight vs Age of the animals"}
ggplot(dat) +
  aes(age, weight, colour = sex) +
  geom_point(shape = 1) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  labs(x = "Age (days)",
       y = "Weight (Kg)",
       colour = "Sex")
```

In R we can use the ```lm()``` function to fit a linear model to this data. Normally we store the results of using the function in an R object and look at it using ``summary()`. We can also get a tidier output using ```get_model_data()``` from the ```sjPlot``` package. Here we make a linear model predicting the animal's weight from their age (a numerical variable) and their sex (a categorical variable).

```{r}
mod1 <- lm(weight ~ age + sex, data = dat)
```

Using ```summary()``` from base-R...


\small
```{r, echo=FALSE}
opt <-options(width = 120)
```

```{r}
summary(mod1)
```

\normalsize

Using ```get_model_data()``` from ```sjPlot``` package and selecting the output columns we want...

```{r, echo=FALSE}
opt <-options(width = 120)
```

\small
```{r}
get_model_data(mod1) %>%
  select(term:p.stars) %>% 
  print()
```
\normalsize

```{r, echo=FALSE}
options(opt)
```

The analysis suggestes that both __age__ and __sex__ are significant predictors of weight. Weight increasing by about `r round(coef(mod1)[["age"]], 2)` Kg per day of age and that males are about `r round(coef(mod1)[["sexmale"]], 1)` Kg heavier than females. Right - now to look at binary (yes/no) outcome data and logistic regression...

\newpage

# Analysing binary data
Binary data is common in epidemiological studies e.g. disease status (diseased or healthy), life (alive or dead) etc. In the example dataset we have the column `status` where animals can be either __healthy__ or __diseased__.
\break
```{r, echo = FALSE}
dat %>% 
  sample_n(6) %>% 
  kable(format = "latex", caption = "Example data")
```
\break
As we know that some animals were treated with an anthelmintic and some were not it may be interesting to look at the number of healthy and diseased animals in the treated and control (untreated) groups.
\break
```{r, echo = FALSE, fig.cap="Disease status vs treatment"}
dat %>% 
  count(treatment, status) %>% 
  spread(status, n) %>% 
  kable(format = "latex")
```
\break
Approximately `r dat %>% filter(treatment == "control") %>% summarise(mean(status == "diseased") * 100) %>% round()`% of the untreated (control) animals were diseased and `r dat %>% filter(treatment == "treated") %>% summarise(mean(status == "diseased") * 100) %>% round()`% of the treated animals were diseased. We can use a 'Fisher's Exact test' to test if the proportion of diseased anaimals in the treatment vs control groups are significantly different. This test also estimates the odds ratio of disease between the two groups. `r margin_note("You can use ?fisher.test in the RStudio console to get help on this function. Here we have also used the with() function to make it easier to use the 'dat' dataset and refer to the columns in the fisher.test() code")`
\break
```{r}
with(dat,
     {{fisher.test(status, treatment)}})
```

The Fisher's exact test reports a p-value, in this case `r with(dat,{{fisher.test(status, treatment)}})$p.value %>% round(3)`. If this is less than your chosen critical p-value you have evidence that the proprtion of diaased animals is significantly different between the treated and untreated (control) animal groups. Fisher's Exact test is useful for comaprions between a binary outcome and a single, binary predciotr. However we may want to look at the association between several predcitors and a binary outcome in one go. This is called __multivariable__ analysis and you'll have met it in linear regression. We can't look at this with a simple test so need to consider regression approaches...

\newpage

# Multivariable analysis

We could convert out `status` column to a number coding healhty animals as __0__ and diseased animals as __1__ and then use conventional linear regression with the variables we want to consider as predictors. The following code will make a new column, `status01` recoding status to a number.


```{r, eval = FALSE}
dat <- dat %>%
  mutate(status01 = as.numeric(status == "diseased"))
```

```{r, echo = FALSE, fig.fullwidth = TRUE}
dat <- dat %>%
  mutate(status01 = as.numeric(status == "diseased"))

dat %>% 
  head(4) %>% 
  kable(format = "latex")
```
\break 

`r margin_note("To understand how this works remember that == in R compares things and if they are the same returns TRUE andf if they aren't returns FALSE. Then as.numeric() will convert FALSE to a 0 and TRUE to 1. It's a handy way of making binary text /factor data into 0's and 1's")`

Now we have converted the binary data into a number (0 or 1) we __could__ try using linear regression. First, to help understand what we are trying here, we can plot the outcome against the treatment group and age of the animals we see this...

```{r, fig.cap = "Status vs Age and treatment", fig.height=3, fig.width=6, out.height='30%'}
ggplot(dat) +
  aes(age, status01, colour = treatment) +
  geom_point(shape = 1) +
  theme_bw(base_size = 16) +
  labs(x = "Age (days)",
       y = "Outcome",
       colour = "Treatment")
```
\break
Linear regression will attempt to fit straight lines of 'prediction' to the control and treatment data.

```{r, echo=FALSE}
opt <-options(width = 120)
```

```{r}
mod_lin <- lm(status01 ~ age + treatment, data = dat)

get_model_data(mod_lin) %>% 
  select(term:p.stars)
```

```{r, echo = FALSE}
options(opt)
```

Before we get too excited by this let's look at the fitted prediction line on the original plot.

```{r, fig.cap = "Status vs Age and treatment (with linear model)", fig.height=3, fig.width=6, fig.fullwidth = TRUE, out.height='30%'}
dat %>% 
  modelr::add_predictions(mod_lin) %>%
  ggplot() +
  aes(age, status01, colour = treatment) +
  geom_point(shape = 1) +
  geom_line(aes(y = pred)) +
  theme_bw(base_size = 16) +
  labs(x = "Age (days)",
       y = "Outcome",
       colour = "Treatment")

```
\break
Our outcome can only be __0__ i.e. __healhty__ or __1__ i.e. __diseased__. However our linear mode is predicting numbers in between. In some circumstances trying to use a linear model to predict binary 0/1/ data will even give values less than zero and more than one. We need to re-think the regression approach so that our model makes sense with an outcome that can only be __0__ or __1__ (or healthy / diseased etc). `r margin_note("Note: In some circumstances using a linear model with a 0/1 outcome is a sensible strategy. It's called a 'linear probability model'. You'll see it used by economists. However we'd suggest you use logistic regression unless you are sure that a linear model with bianry outcokes is correct!")`

We do this in logistic regression by making a model that estimates the __probability__ of an animal having the outcome. Remembering that probabilities can take a value between __0__ and __1__ we modify the linear regression model so that its predictions lie only between 0 and 1. As linear regression can produce numbers ranging from hugely negative (negative infinity in fact) to hugely postivie (positive infinity) we use a __logit__ function.
\break

Here's a plot of what it does...

`r margin_note("Mathematically the regression value is $log_e(\\frac{P}{1-P})$ where $P$ is the probability of the outsome. Understanding this isn't critical but if you want to read more about this look at the logistic regression chapter in the Dahoo book (see references)")`

```{r, echo = FALSE, fig.height=3, fig.width=4}
tibble(y = seq(-10, 10, 0.1),
       prob = boot::inv.logit(y)) %>% 
  ggplot() +
  aes(y, prob) +
  geom_line(colour = purple, lwd = 2) +
  labs(x = "linear regression value",
       y = "probability") +
  theme_bw(base_size = 12)
```
\break
It's turning a number (in this case from -10 to 10) into a probability that can __only__ range from 0 to 1. If we expanded the range of numbers to be from minus infinity to plus infinity the probability would still only go from 0 to 1. The number is infact the 'log' odds of the event. Logs of odds can range from - infinity to + infinity - because odds can range from 0 to infinity [Don't worry if you don't follow that maths!].


#The logistic regression models

So we use this logistic function together with a linear regression model to build a model that predicts the probability of an oucome given one or more precitor variables. The linear model bit generates a number from - infinity to + infinity and this is turned into a probaility (0 - 1). So a model for disease based on age and treatment mathematically looks like this...

$$ log_e(\frac{prob}{1 - prob}) \sim \beta_0 + \beta_1 age + \beta_2 treatment) $$


Or in words: The log of the odds of an animal being diseased are modelled by a linear combination of the predictor variables.

Let's look at a worked example usiong the data we have loaded...

# Worked example in R

First a reminder about the data structure...

```{r, eval= FALSE}
head(dat)
```


```{r, echo = FALSE}
head(dat) %>% 
  kable(format = "latex")
```
\break

We specifiy logistic regression models in R using the `glm()` function. Just like in a linear model we have the outcome on the left hand side of the formula and the predictors on the right. We need to add an argument `family = binomial` to tell R to do logistic regression. We store the model in an object. Just like in linear regression...

```{r}
mod_disease <- glm(status01 ~ treatment + age, family = binomial, data = dat)
```

We can look at a summary of the model output using `summary()`...

```{r, eval = FALSE}
print(summary(mod_disease), digits = 3)
```

\small

```{r, echo = FALSE}
print(summary(mod_disease), digits = 3)
```

\normalsize

# odds ratios

Because we used some clever maths to convert the numbers from a linear model to a probability scale of zero to one the raw estimates from our model are actually 'log odds ratios'. In epidmeiology we want to look at simple odds ratios. i.e.

$$\frac{odds\ of\ outcome\ if\ have\ factor}{odds\ of\ outcome\ if\ dont\ have\ factor}$$

So we get odds ratios by 'inverse logging' the raw estimates from the model. If we use the `get_model_data()` function from the `sjPlot` package it will automatically convert the estimates from a logistic regression model into odds ratios by inverse logging them...

\small
```{r}
get_model_data(mod_disease) %>% 
  select(term:p.stars)
```
\normalsize

Compare the results with the raw estimates from `summary(mod_disease)` to see what's happened. For example, from the raw results `r coef(mod_disease)[[2]] %>% round(3)` has become `r exp(coef(mod_disease)[[2]]) %>% round(3)` as it has gone from `log(odds-ratio)` to `odds_ratio` etc.

# Interpreting the odds ratios

Odds ratios __multiply__ when we interpret them. Let's look at what this means for categorical and numerical predictors...

## Categorical predictors

The odds ratio estimate here means 'how many times greater the odds of outcome are __if__ the risk factor (etc) is present'. So for the treatment variable (which can be control or treatment) the odds of disease if treated are `r round(exp(coef(mod_disease))[[2]], 3)` __times greater__ than if untreated (control).

## Numerical predictors

And here they mean 'hw many times greater the odds of outcome are for __each unit change__ in the variable'. So for the age variable the odds of disease are `r round(exp(coef(mod_disease))[[3]], 3)` __times greater__ for each day older. So for 3 days older the odds of being diseased are `r round(exp(coef(mod_disease))[[3]], 3)` x `r round(exp(coef(mod_disease))[[3]], 3)` x `r round(exp(coef(mod_disease))[[3]], 3)` $\simeq$ `r round(exp(coef(mod_disease))[[3]], 3)^3 %>% round(3)` greater.

# Things to watch out for

## Factor levels

__How does R know if you are predicting 'healthy' or 'diseased'?__ In our example above we usd the code `as.numeric(status == "diseased")` to convert 'diseased' into 1 and 'healthy' into 0. That way 'diseased became our outcome of interest. There are other ways such as using `... glm(status == "diseased" ~ ...` in the model code. Whatever you do make sure youy are predicting the outcome of interest. otherwise your odds ratios estimates will be opposite!

Also when you have a categorical predictor the odds ratio estimate will be the odds of the outcome if the predictor takes a certain level compared to it being the 'reference' level. Imagine a simple model predictong disease status from regio using the example data...

```{r}
mod_region  <- glm(status01 ~ region,
                   family = binomial,
                   data = dat)

get_model_data(mod_region) %>% 
  select(term:p.stars)
```

The estimates for regions B to D are the odds of diseased if in that region compared to the odds of disease in region A. region A is the __reference__ region. By deafult R will choose the first level in a categorical or factor variable. Unles you instruct otherwise these will normally be the first in alhpaebtical order. Sometimes it makes sense to choose a diffeent reference level. There are several thing to comnsider but generally we'd advise taht you choose a refrenece level that has plamnty of observations. If three levels had 100 obseravtions and one level had just 3 it would be a poor choice as a reference level as we would be comparing disaese rates to a level with great uncertainty. Also try and choose a refrence level that readers will find useful. If you were lookignm at breed risks then choosing a common, well-know breed as a refrence level makes sense and helps your readers. To change the refrence level use this code...

```{r}
# Change the reference level in region to 'C'
# uses the fct_relevel function from tidyverse/forcats package

dat <- dat %>% 
  mutate(region = fct_relevel(region, "C"))


mod_region  <- glm(status01 ~ region,
                   family = binomial,
                   data = dat)

get_model_data(mod_region) %>% 
  select(term:p.stars)
```

You'll see that the regions are all comapred back to region C!

## Perfect predictors

Sometimes a variable is a prefect predictor of the outcome of interest. An example would be where all the animals of one breed or sex were diseased (none were healthy). If we use conventional logistic regression modelling to estimate the odds ratios we may see strange results. A typical warning sign is that the standard erros are hundres and thousands and the confidence intervals for the odds ratio are massive. If you see this do a table to check the realtionship between outcome and predictor. If you have a perfect predictor you either need to seek further statistical help or do some in-depth reading. Teh `logistf` package in R can sometimes help but we'd advise you carefully read before using it. Here's an article on it (https://www.r-bloggers.com/example-8-15-firth-logistic-regression/)   

## Non-linear predictors

For numerical predictors we are assuming that for each unit increase in the numerical predictor the odds of the outcome multiply the same amount. E.g. for every extral metre of altitude the odds of disease increase or decrease by X times where X is the odds ratio from out model output. If we use numerical predcitor in out model we should check this as somethimes it's not the case. For example a drug with toxic side effects may be associated with decreasing risk of mortality as the dose increases from zero due to its beneficial effects but then at a threshold the mortality rate might increase due to toxic effects. Let's investigate this with the example data lookign at the 'supp' column which records the dose of supplement given to our animals. First we'll crate a simple logustic regression model with treatment and supplement...

\small
```{r}
mod_sup1 <- glm(status01 ~ treatment + supp, family = binomial, data = dat)

get_model_data(mod_sup1) %>% 
  select(term:p.stars)
```
\normalsize

The results suggest no significant realtionship between supplement and disease (when correctd for treratment). Howver we should look into this and check that theres not a non-linear realtionship between outsome and supplemntation. As a quick check we can use some R code to break the 'supp' column into 5 bands and caclaulte the proportion of diseased animals in each band. We use the `cut_number()` function to create the bands..


```{r}
# add a column with supplement bands
dat <- dat %>% 
  mutate(supp_cut = cut_number(supp, 5))

# calculate proportion diseased in each band
dat %>% 
  group_by(supp_cut) %>% 
  summarise(percent_diseased = mean(status == "diseased",
                                    na.rm = TRUE) * 100)
```

```{r, echo = FALSE, fig.margin=TRUE, fig.width=5, fig.height=4, fig.cap = "Percentage diseased for\neach supplement band"}

dat %>% 
  group_by(supp_cut) %>% 
  summarise(percent_diseased = mean(status == "diseased",
                                    na.rm = TRUE) * 100) %>% 
  ggplot() +
  aes(x = supp_cut, y = percent_diseased) +
  geom_col(width = 0.1) +
  labs(x = "Supplement dose band",
       y = "Percentage disease")
```


That looks suspicious. As supplement levels increase the precenatge of diseased animasl decreases then increases. This suggests putting 'supp' into the model as a simple linear term may bot be sensible. There's lots of startegies we could use including sophisticated mathematical ones called genralised additive models (GAMS) but a easy solution is to simply use the supplement bands as categories in the model rather than using the numerical value. That way we can capture different responses across the range of supplement values...


\small
```{r}
mod_sup2 <- glm(status01 ~ treatment + supp_cut,
                family = binomial,
                data = dat)


get_model_data(mod_sup2) %>% 
  select(term:p.stars)
```
\normalsize

By default the lowest supplement band `r levels(dat$supp_cut)[[1]]` is used as the reference level. We can see the odds ratios of disease in the other bands compared to the reference level. They decease as supplement increases at first then increase again. It suggests there's a 'U' shaped realtionship between odds of disease and supplement level. Certainly not a straight line.

# Model selection

So far we have used logistic egression models with arbitrary predictor valriables from our dataset. However in a real modelling exercise we may have lots of preditors to choose from and need to have a sensible methodology for choosing which variables to include in our models. This area of 'model slection' or 'variable selection' is complex and causes arguments amongst statisticians and epidemiologists. See Frank Harrell's book 'Regression modeling strategies' [ISBN-13: 978-3319194240] for in depth advice. This book is available as an eBook from the University of Edinbvurgh library.

```{r, echo=FALSE}
options(opt)
```
